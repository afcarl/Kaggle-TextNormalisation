{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import gc\n",
    "import pickle\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import xgboost as xgb\n",
    "from sklearn.model_selection import train_test_split\n",
    "import inflect\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from collections import defaultdict, Counter\n",
    "import feather\n",
    "from num2words import num2words \n",
    "import re\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_classify_train_data(np_file,csv_file):\n",
    "    if os.path.exists(np_file) == True:\n",
    "       temp = np.load(np_file)\n",
    "       return temp['x_train'],temp['y_train'],temp['label']\n",
    "    else:\n",
    "        num_features = 9 #每个 word 取前 5 后 4 个字符来编码\n",
    "        train=pd.read_csv(csv_file)\n",
    "        tmp=pd.factorize(train['class'])\n",
    "        y_train,label=tmp[0].astype(np.int8),tmp[1].values\n",
    "        num_train=len(y_train)\n",
    "        train['before']=train['before'].astype(np.str)\n",
    "        x_train=np.zeros([num_train,num_features],np.int8)\n",
    "        feature=np.zeros([num_train,7],np.int8)# 人工提取的特征\n",
    "        list1=('a','e','i','o','u')# 元音\n",
    "        list2=('+','-','*','//','%')# 数学运算符\n",
    "        for word,row in zip(train['before'].values,range(num_train)):\n",
    "            if(len(word)>=num_features):\n",
    "                for c,col in zip(word[:5],range(5)):\n",
    "                    x_train[row,col]=ord(c)\n",
    "                for c,col in zip(word[-4:],range(5,9)):\n",
    "                    x_train[row,col]=ord(c)\n",
    "            else:\n",
    "                for c,col in zip(word,range(num_features)):\n",
    "                    x_train[row,col]=ord(c)\n",
    "            feature[row, 3] =len(word) # 统计字符串的长度\n",
    "            dotflag=0\n",
    "            for c in word:\n",
    "                if c.isdigit():feature[row,0]+=1# 统计数字的个数\n",
    "                if c.isupper():feature[row,1]+=1# 统计大写字母的个数\n",
    "                if c.isalnum()!=True:feature[row,2]+=1# 统计非字母和数字的个数\n",
    "                if c in list1:feature[row,4]+=1# 统计元音的个数\n",
    "                if c=='.': dotflag=1\n",
    "                elif dotflag==1:#  . 后面跟字母置 1 ，数字置 2，其他置 3\n",
    "                    dotflag = 0\n",
    "                    if c.isdigit():feature[row,5]+=10\n",
    "                    elif c.isalpha():feature[row,5]+=100\n",
    "                    else:feature[row,5]+=1000\n",
    "                if c in list2:feature[row,6]+=1# 统计数学运算符的个数\n",
    "\n",
    "        # 掐头去尾，结合上文 2 单词，下文 1 个单词\n",
    "        num_train-=3\n",
    "        y_train=y_train[2:-1]\n",
    "        x_train=np.concatenate((x_train[:-3],x_train[1:-2],x_train[2:-1],x_train[3:],feature[2:-1]),axis=1)\n",
    "        np.savez(np_file,x_train=x_train, y_train=y_train, label=label)\n",
    "        return x_train, y_train, label\n",
    "\n",
    "def get_classify_test_data(np_file,csv_file):\n",
    "    test=pd.read_csv(csv_file)\n",
    "    if os.path.exists(np_file) == True:\n",
    "       temp = np.load(np_file)\n",
    "       x_test=temp['x_test']\n",
    "    else:\n",
    "        num_features = 9 #每个 word 取前 5 后 4 个字符来编码\n",
    "        human_feature=7 #人工提取7个特征\n",
    "        num_test=len(test)\n",
    "        test['before']=test['before'].astype(np.str)\n",
    "        x_test=np.zeros([num_test,num_features],np.int8)\n",
    "        feature=np.zeros([num_test,human_feature],np.int8)# 人工提取的特征\n",
    "        list1=('a','e','i','o','u')# 元音\n",
    "        list2=('+','-','*','//','%')# 数学运算符\n",
    "        for word,row in zip(test['before'].values,range(num_test)):\n",
    "            if(len(word)>=num_features):\n",
    "                for c,col in zip(word[:5],range(5)):\n",
    "                    x_test[row,col]=ord(c)\n",
    "                for c,col in zip(word[-4:],range(5,9)):\n",
    "                    x_test[row,col]=ord(c)\n",
    "            else:\n",
    "                for c,col in zip(word,range(num_features)):\n",
    "                    x_test[row,col]=ord(c)\n",
    "            feature[row, 3] =len(word) # 统计字符串的长度\n",
    "            dotflag=0\n",
    "            for c in word:\n",
    "                if c.isdigit():feature[row,0]+=1# 统计数字的个数\n",
    "                if c.isupper():feature[row,1]+=1# 统计大写字母的个数\n",
    "                if c.isalnum()!=True:feature[row,2]+=1# 统计非字母和数字的个数\n",
    "                if c in list1:feature[row,4]+=1# 统计元音的个数\n",
    "                if c=='.': dotflag=1\n",
    "                elif dotflag==1:#  . 后面跟字母置 1 ，数字置 2，其他置 3\n",
    "                    dotflag = 0\n",
    "                    if c.isdigit():feature[row,5]+=10\n",
    "                    elif c.isalpha():feature[row,5]+=100\n",
    "                    else:feature[row,5]+=1000\n",
    "                if c in list2:feature[row,6]+=1# 统计数学运算符的个数\n",
    "\n",
    "        # 开头补上2个单词,结尾补上1个单词，结合上文 2 单词，下文 1 个单词\n",
    "        x_test = np.concatenate((np.zeros([2,num_features],np.int8),x_test,np.zeros([1,num_features],np.int8)),axis=0)\n",
    "        feature = np.concatenate((np.zeros([2,human_feature],np.int8),feature,np.zeros([1,human_feature],np.int8)),axis=0)\n",
    "        x_test=np.concatenate((x_test[:-3],x_test[1:-2],x_test[2:-1],x_test[3:],feature[2:-1]),axis=1)\n",
    "        np.savez(np_file,x_test=x_test)\n",
    "    return test, x_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__=='__main__':\n",
    "    prehead=''\n",
    "    train_data_csv='en_train.csv'\n",
    "    classify_train_file='classify_train.npz'\n",
    "    xgb_model='xgb_model.dat'\n",
    "    test_data_csv='en_test_2.csv'\n",
    "    classify_test_file='classify_test.npz'\n",
    "    xgb_model2='xgb_model2.dat'\n",
    "    classify_test_file2='classify_test2.npz'\n",
    "\n",
    "    # 训练模型\n",
    "    x_train,y_train,label=get_classify_train_data(prehead+classify_train_file,prehead+train_data_csv)\n",
    "    print(x_train.shape)\n",
    "    dtrain = xgb.DMatrix(x_train, label=y_train)\n",
    "    watchlist = [(dtrain, 'train')]\n",
    "    param = {\n",
    "        'eta': 0.3,\n",
    "        'max_depth':10,\n",
    "        'objective':'multi:softmax',\n",
    "        'num_class':len(label),\n",
    "        'eval_metric':'merror',\n",
    "        'subsample': 1,\n",
    "        'colsample_bytree': 1,\n",
    "        'silent':1,\n",
    "        'seed':0,\n",
    "    }\n",
    "    num_boost_rounds=10\n",
    "    model = xgb.train(param, dtrain, num_boost_rounds, watchlist,verbose_eval=1)\n",
    "    print('save model ',xgb_model2)\n",
    "    pickle.dump(model,open(xgb_model2,'wb'))# 保存模型\n",
    "    del x_train,y_train\n",
    "    #gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# xgb\n",
    "params = {'eta': 0.1, 'max_depth': 4, 'subsample': 0.9, 'colsample_bytree': 0.9, \n",
    "          'objective': 'binary:logistic', 'eval_metric': 'accuracy', 'silent': False}\n",
    "\n",
    "X = train[['class', 'before']]\n",
    "X['class'] = X['class'].astype('category')\n",
    "X['before'] = X['before'].astype('category')\n",
    "y = train['after'].values\n",
    "#y = y.astype('category')\n",
    "test['class'] = ''\n",
    "test['class'] = test['class'].astype('category')\n",
    "test['before'] = test['before'].astype('category')\n",
    "\n",
    "sub = (test.sentence_id.astype(str) + '_' + test.token_id.astype(str)).to_frame()\n",
    "sub['target']=''\n",
    "\n",
    "sub_train = (train.sentence_id.astype(str) + '_' + train.token_id.astype(str)).to_frame()\n",
    "sub_train['target']=''\n",
    "nrounds= 200 #10**6  # need to change to 2000\n",
    "kfold = 5  # need to change to 5\n",
    "\n",
    "d_train = xgb.DMatrix(X, y) \n",
    "d_valid = xgb.DMatrix(test) \n",
    "xgb_model = xgb.train(params, d_train, nrounds, early_stopping_rounds=10, \n",
    "                       maximize=True, verbose_eval=10)\n",
    "sub['target'] += xgb_model.predict(xgb.DMatrix(test[features].values), \n",
    "                    ntree_limit=xgb_model.best_ntree_limit+50) / (kfold)\n",
    "\n",
    "sub_train['target'] += xgb_model.predict(xgb.DMatrix(train[features].values), \n",
    "                    ntree_limit=xgb_model.best_ntree_limit+50) / (kfold)\n",
    "\n",
    "gc.collect()\n",
    "sub.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prehead=''\n",
    "train_data_csv='en_train.csv'\n",
    "classify_train_file='classify_train.npz'\n",
    "xgb_model='xgb_model.dat'\n",
    "test_data_csv='en_test_2.csv'\n",
    "classify_test_file='classify_test.npz'\n",
    "xgb_model2='xgb_model2.dat'\n",
    "classify_test_file2='classify_test2.npz'\n",
    "train = pd.read_csv('en_train.csv')\n",
    "model = pickle.load(open('xgb_model2.dat', 'rb'))\n",
    "test_data_csv = 'en_test_2.csv'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "    # 预测 test 上的 class\n",
    "    #model = pickle.load(open(xgb_model2, \"rb\"))\n",
    "    test,x_test=get_classify_test_data(prehead+classify_test_file,prehead+test_data_csv)\n",
    "    print(x_test.shape)\n",
    "    dtest = xgb.DMatrix(x_test)\n",
    "    pred = model.predict(dtest)\n",
    "    pred = [label[int(x)] for x in pred]\n",
    "    test['class']=pred\n",
    "    test.to_csv(os.path.join(prehead, 'test_pred_class_new.csv'))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(test.head())\n",
    "test['class'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sub.to_csv(base_path+'test_sub_xgb.csv', index=False)\n",
    "sub_train.to_csv(base_path+'train_sub_xgb_new.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = train[['class', 'before']]\n",
    "X['class'] = X['class'].astype('category')\n",
    "X['before'] = [''.join(str(ord(c)) for c in s) for s in X['before']]\n",
    "y = train['after'].values\n",
    "y = y.map(ord)\n",
    "\n",
    "test['class'] = test['class'].astype('category')\n",
    "test['before'] = test['before'].map(ord)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Defining dictionary, rules and running predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Read the train file, google dataset, test file with predicted classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_csv('en_train.csv')\n",
    "test = pd.read_csv('test_pred_class_new.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Kerem\n",
    "# d = train.groupby(['before', 'after']).size()\n",
    "# d = d.reset_index().sort_values(0, ascending=False)\n",
    "# d = d.loc[d['before'].drop_duplicates(keep='first').index]\n",
    "# d = d.loc[d['before'] != d['after']]\n",
    "# d = d.set_index('before')['after'].to_dict()\n",
    "# test = pd.read_csv('test_class_kerem.csv')\n",
    "# test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #Kerem\n",
    "# test['random'] = ''\n",
    "# test = test[['random','sentence_id', 'token_id', 'class']]\n",
    "# test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "d = defaultdict(list)\n",
    "train_list = [(train.iloc[i,3],train.iloc[i,4]) for i in range(train.shape[0])]\n",
    "for k,v in train_list:\n",
    "    d[k].append(v)\n",
    "    \n",
    "train_dict = {}\n",
    "for key in d:\n",
    "    c = Counter(d[key]).most_common(1)[0][0]\n",
    "    train_dict[key] = c\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'.'"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dict['.']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# d1 = defaultdict(list)\n",
    "\n",
    "# google_dataset = os.listdir('en_with_types/')\n",
    "# for file in google_dataset:\n",
    "#     data = pd.read_csv('en_with_types/' + file, sep = '\\t', error_bad_lines= False, quoting=3)\n",
    "#     print(file)\n",
    "#     data = data[data.iloc[:,0] != 'PUNCT']\n",
    "#     data = data[data.iloc[:,0] != 'ELECTRONIC']    \n",
    "#     data = data[data.iloc[:,0] != '<eos>']   \n",
    "#     data = data.drop(data.columns[0], axis=1)\n",
    "#     #print(data.head())\n",
    "    \n",
    "#     data_list = [(data.iloc[i,0],data.iloc[i,1]) for i in range(data.shape[0])]\n",
    "#     for k,v in data_list:\n",
    "#         #print(data_list)[1:10]\n",
    "#         #print(k)\n",
    "#         #print(v)\n",
    "#         d1[k].append(v)\n",
    "#         #print(d1[k])\n",
    "        \n",
    "# counter_dict = {}\n",
    "# for key in d1:\n",
    "#     c = Counter(d1[key]).most_common(1)[0][0]\n",
    "#     counter_dict[key] = c\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pickle\n",
    "\n",
    "# f = open(\"dict.pkl\",\"wb\")\n",
    "# pickle.dump(counter_dict,f)\n",
    "# f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>sentence_id</th>\n",
       "      <th>token_id</th>\n",
       "      <th>before</th>\n",
       "      <th>class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>943352</th>\n",
       "      <td>943352</td>\n",
       "      <td>69066</td>\n",
       "      <td>0</td>\n",
       "      <td>BioLib.cz</td>\n",
       "      <td>ELECTRONIC</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>943563</th>\n",
       "      <td>943563</td>\n",
       "      <td>69081</td>\n",
       "      <td>0</td>\n",
       "      <td>Barbados.gov.bb</td>\n",
       "      <td>ELECTRONIC</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>943922</th>\n",
       "      <td>943922</td>\n",
       "      <td>69104</td>\n",
       "      <td>7</td>\n",
       "      <td>Snopes.com</td>\n",
       "      <td>ELECTRONIC</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>947664</th>\n",
       "      <td>947664</td>\n",
       "      <td>69379</td>\n",
       "      <td>6</td>\n",
       "      <td>Reginagallery.com</td>\n",
       "      <td>ELECTRONIC</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>950459</th>\n",
       "      <td>950459</td>\n",
       "      <td>69589</td>\n",
       "      <td>3</td>\n",
       "      <td>foreverastro.com</td>\n",
       "      <td>ELECTRONIC</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>951175</th>\n",
       "      <td>951175</td>\n",
       "      <td>69639</td>\n",
       "      <td>2</td>\n",
       "      <td>Fussballdaten.de</td>\n",
       "      <td>ELECTRONIC</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>952168</th>\n",
       "      <td>952168</td>\n",
       "      <td>69714</td>\n",
       "      <td>5</td>\n",
       "      <td>Historywc.rootsweb.com</td>\n",
       "      <td>ELECTRONIC</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>952475</th>\n",
       "      <td>952475</td>\n",
       "      <td>69737</td>\n",
       "      <td>3</td>\n",
       "      <td>::</td>\n",
       "      <td>ELECTRONIC</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>954139</th>\n",
       "      <td>954139</td>\n",
       "      <td>69854</td>\n",
       "      <td>19</td>\n",
       "      <td>http://dos.myflorida.com/elections/data-statis...</td>\n",
       "      <td>ELECTRONIC</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>954640</th>\n",
       "      <td>954640</td>\n",
       "      <td>69895</td>\n",
       "      <td>0</td>\n",
       "      <td>Changsha.com</td>\n",
       "      <td>ELECTRONIC</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        Unnamed: 0  sentence_id  token_id  \\\n",
       "943352      943352        69066         0   \n",
       "943563      943563        69081         0   \n",
       "943922      943922        69104         7   \n",
       "947664      947664        69379         6   \n",
       "950459      950459        69589         3   \n",
       "951175      951175        69639         2   \n",
       "952168      952168        69714         5   \n",
       "952475      952475        69737         3   \n",
       "954139      954139        69854        19   \n",
       "954640      954640        69895         0   \n",
       "\n",
       "                                                   before       class  \n",
       "943352                                          BioLib.cz  ELECTRONIC  \n",
       "943563                                    Barbados.gov.bb  ELECTRONIC  \n",
       "943922                                         Snopes.com  ELECTRONIC  \n",
       "947664                                  Reginagallery.com  ELECTRONIC  \n",
       "950459                                   foreverastro.com  ELECTRONIC  \n",
       "951175                                   Fussballdaten.de  ELECTRONIC  \n",
       "952168                             Historywc.rootsweb.com  ELECTRONIC  \n",
       "952475                                                 ::  ELECTRONIC  \n",
       "954139  http://dos.myflorida.com/elections/data-statis...  ELECTRONIC  \n",
       "954640                                       Changsha.com  ELECTRONIC  "
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test[test['class'] == 'ELECTRONIC'].tail(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<self>'"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# f = open(\"dict.pkl\",\"wb\")\n",
    "# pickle.dump(counter_dict,f)\n",
    "# f.close()\n",
    "\n",
    "p1 = pickle.load(open('dict.pkl', 'rb'))\n",
    "\n",
    "p1['the']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import os\n",
    "\n",
    "# p = {} # scores is an empty dict already\n",
    "# target = 'dict.pkl'\n",
    "# if os.path.getsize(target) > 0:      \n",
    "#     with open(target, \"rb\") as f:\n",
    "#         unpickler = pickle.Unpickler(f)\n",
    "#         # if file is not empty scores will be equal\n",
    "#         # to the value unpickled\n",
    "#         p = unpickler.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rom_to_int(string):\n",
    "\n",
    "    table=[['M',1000],['CM',900],['D',500],['CD',400],['C',100],['XC',90],['L',50],['XL',40],['X',10],['IX',9],['V',5],['IV',4],['I',1]]\n",
    "    returnint=0\n",
    "    for pair in table:\n",
    "\n",
    "\n",
    "        continueyes=True\n",
    "\n",
    "        while continueyes:\n",
    "            if len(string)>=len(pair[0]):\n",
    "\n",
    "                if string[0:len(pair[0])]==pair[0]:\n",
    "                    returnint+=pair[1]\n",
    "                    string=string[len(pair[0]):]\n",
    "\n",
    "                else: continueyes=False\n",
    "            else: continueyes=False\n",
    "\n",
    "    return returnint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "## For classes:\n",
    "p = inflect.engine()\n",
    "def plain(x):\n",
    "    return x\n",
    "\n",
    "def punct(x):\n",
    "    return x\n",
    "\n",
    "def cardinal(x):\n",
    "    try:\n",
    "        if re.match('.*[A-Za-z]+.*', x):\n",
    "            return x\n",
    "        x = re.sub(',', '', x, count = 10)\n",
    "\n",
    "        if(re.match('.+\\..*', x)):\n",
    "            x = p.number_to_words(float(x))\n",
    "        elif re.match('\\..*', x): \n",
    "            x = p.number_to_words(float(x))\n",
    "            x = x.replace('zero ', '', 1)\n",
    "        else:\n",
    "            x = p.number_to_words(int(x))\n",
    "        x = x.replace('zero', 'o')    \n",
    "        x = re.sub('-', ' ', x, count=10)\n",
    "        x = re.sub(' and','',x, count = 10)\n",
    "        return x\n",
    "    except:\n",
    "        return x\n",
    "\n",
    "def verbatim(x):\n",
    "    return(x)\n",
    "\n",
    "\n",
    "dict_mon = {'jan': \"January\", \"feb\": \"February\", \"mar \": \"march\", \"apr\": \"april\", \"may\": \"may \",\"jun\": \"june\", \"jul\": \"july\", \"aug\": \"august\",\"sep\": \"september\",\n",
    "            \"oct\": \"october\",\"nov\": \"november\",\"dec\": \"december\", \"january\":\"January\", \"february\":\"February\", \"march\":\"march\",\"april\":\"april\", \"may\": \"may\", \n",
    "            \"june\":\"june\",\"july\":\"july\", \"august\":\"august\", \"september\":\"september\", \"october\":\"october\", \"november\":\"november\", \"december\":\"december\"}\n",
    "def date(key):\n",
    "    dict_mon = {'jan': \"January\", \"feb\": \"February\", \"mar \": \"march\", \"apr\": \"april\", \"may\": \"may \",\"jun\": \"june\", \"jul\": \"july\", \"aug\": \"august\",\"sep\": \"september\",\n",
    "            \"oct\": \"october\",\"nov\": \"november\",\"dec\": \"december\", \"january\":\"January\", \"february\":\"February\", \"march\":\"march\",\"april\":\"april\", \"may\": \"may\", \n",
    "            \"june\":\"june\",\"july\":\"july\", \"august\":\"august\", \"september\":\"september\", \"october\":\"october\", \"november\":\"november\", \"december\":\"december\"}\n",
    "\n",
    "\n",
    "    v =  key.split('-')\n",
    "    if len(v)==3:\n",
    "        if v[1].isdigit():\n",
    "            try:\n",
    "                date = datetime.strptime(key , '%Y-%m-%d')\n",
    "                text = 'the '+ p.ordinal(p.number_to_words(int(v[2]))).replace('-',' ')+' of '+datetime.date(date).strftime('%B')\n",
    "                if int(v[0])>=2000 and int(v[0]) < 2010:\n",
    "                    text = text  + ' '+cardinal(v[0])\n",
    "                else: \n",
    "                    text = text + ' ' + cardinal(v[0][0:2]) + ' ' + cardinal(v[0][2:])\n",
    "            except:\n",
    "                text = key\n",
    "            return text.lower()    \n",
    "    else:   \n",
    "        v = re.sub(r'[^\\w]', ' ', key).split()\n",
    "        if v[0].isalpha():\n",
    "            try:\n",
    "                if len(v)==3:\n",
    "                    text = dict_mon[v[0].lower()] + ' '+ p.ordinal(p.number_to_words(int(v[1]))).replace('-',' ')\n",
    "                    if int(v[2])>=2000 and int(v[2]) < 2010:\n",
    "                        text = text  + ' '+cardinal(v[2])\n",
    "                    else: \n",
    "                        text = text + ' ' + cardinal(v[2][0:2]) + ' ' + cardinal(v[2][2:])   \n",
    "                elif len(v)==2:\n",
    "\n",
    "                    if int(v[1])>=2000 and int(v[1]) < 2010:\n",
    "                        text = dict_mon[v[0].lower()]  + ' '+ cardinal(v[1])\n",
    "                    else: \n",
    "                        if len(v[1]) <=2:\n",
    "                            text = dict_mon[v[0].lower()] + ' ' + cardinal(v[1])\n",
    "                        else:\n",
    "                            text = dict_mon[v[0].lower()] + ' ' + cardinal(v[1][0:2]) + ' ' + cardinal(v[1][2:])\n",
    "                else: text = key\n",
    "            except: text = key\n",
    "            return text.lower()\n",
    "        else: \n",
    "            key = re.sub(r'[^\\w]', ' ', key)\n",
    "            v = key.split()\n",
    "            try:\n",
    "                date = datetime.strptime(key , '%d %b %Y')\n",
    "                text = 'the '+ p.ordinal(p.number_to_words(int(v[0]))).replace('-',' ')+' of '+ dict_mon[v[1].lower()]\n",
    "                if int(v[2])>=2000 and int(v[2]) < 2010:\n",
    "                    text = text  + ' '+cardinal(v[2])\n",
    "                else: \n",
    "                    text = text + ' ' + cardinal(v[2][0:2]) + ' ' + cardinal(v[2][2:])\n",
    "            except:\n",
    "                try:\n",
    "                    date = datetime.strptime(key , '%d %B %Y')\n",
    "                    text = 'the '+ p.ordinal(p.number_to_words(int(v[0]))).replace('-',' ')+' of '+ dict_mon[v[1].lower()]\n",
    "                    if int(v[2])>=2000 and int(v[2]) < 2010:\n",
    "                        text = text  + ' '+cardinal(v[2])\n",
    "                    else: \n",
    "                        text = text + ' ' + cardinal(v[2][0:2]) + ' ' + cardinal(v[2][2:])\n",
    "                except:\n",
    "                    try:\n",
    "                        date = datetime.strptime(key , '%d %m %Y')\n",
    "                        text = 'the '+ p.ordinal(p.number_to_words(int(v[0]))).replace('-',' ')+' of '+datetime.date(date).strftime('%B')\n",
    "                        if int(v[2])>=2000 and int(v[2]) < 2010:\n",
    "                            text = text  + ' '+cardinal(v[2])\n",
    "                        else: \n",
    "                            text = text + ' ' + cardinal(v[2][0:2]) + ' ' + cardinal(v[2][2:])\n",
    "                    except:\n",
    "                        try:\n",
    "                            date = datetime.strptime(key , '%d %m %y')\n",
    "                            text = 'the '+ p.ordinal(p.number_to_words(int(v[0]))).replace('-',' ')+' of '+datetime.date(date).strftime('%B')\n",
    "                            v[2] = datetime.date(date).strftime('%Y')\n",
    "                            if int(v[2])>=2000 and int(v[2]) < 2010:\n",
    "                                text = text  + ' '+cardinal(v[2])\n",
    "                            else: \n",
    "                                text = text + ' ' + cardinal(v[2][0:2]) + ' ' + cardinal(v[2][2:])\n",
    "                        except:text = key\n",
    "            return text.lower() \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# def date(x):\n",
    "#     try:\n",
    "#         x = re.sub('\\.','',x )\n",
    "#         if re.match('^[0-9]+$', x):\n",
    "#             return re.sub('-', ' ', p.number_to_words(int(x))) \n",
    "#         elif re.match('.*[A-Za-z]+.*',x):\n",
    "#             y = re.sub(',','',x)\n",
    "#             y = y.split(' ')\n",
    "#             result_string = 'the '\n",
    "#             two_dig = [w for w in y if len(w) <= 2]\n",
    "#             if len(two_dig)>0:\n",
    "#                 num = int(str(two_dig[0]))\n",
    "#                 num = re.sub('-', ' ',  num2words(num, ordinal=True))\n",
    "#                 result_string = result_string + num + ' of '\n",
    "#             month = [w for w in y if re.match('^[A-Za-z]+$', w)]\n",
    "#             result_string = result_string  + str(month[0])\n",
    "#             y = [w for w in y if w not in month]\n",
    "#             year = [w for w in y if len(w) == 4]\n",
    "#             year1 = (str(year[0][0:2]))\n",
    "#             year2 = (str(year[0][2:4]))\n",
    "#             year1 = re.sub('-', ' ', p.number_to_words(int(year1)))\n",
    "#             year2 = re.sub('-', ' ', p.number_to_words(int(year2)))\n",
    "#             result_string = result_string + ' ' + year1 + ' ' + year2\n",
    "#             return result_string\n",
    "#         else:\n",
    "#             result_string = 'the '\n",
    "#             y = re.sub('-', ' ', x)\n",
    "#             y = y.split(' ')\n",
    "#             num = int(str(y[0]))\n",
    "#             num = re.sub('-', ' ',  num2words(num, ordinal=True))\n",
    "#             result_string = result_string + num + ' of '\n",
    "#             month = y[1]\n",
    "#             result_string = result_string  + str(month[0])\n",
    "#             year = [w for w in y if len(w) == 4]\n",
    "#             year1 = (str(year[0][0:2]))\n",
    "#             year2 = (str(year[0][2:4]))\n",
    "#             year1 = re.sub('-', ' ', p.number_to_words(int(year1)))\n",
    "#             year2 = re.sub('-', ' ', p.number_to_words(int(year2)))\n",
    "#             result_string = result_string + ' ' + year1 + ' ' + year2\n",
    "#             return result_string\n",
    "#     except:\n",
    "#         return x\n",
    "    \n",
    "def measure(x):\n",
    "    try:\n",
    "        x = re.sub(',', '', count=10)\n",
    "        x = x.split(' ')\n",
    "        replacement = {'MB': 'megabyte', 'MW': 'megawatt','ft':'feet', 'km':'kilometers', 'mm': 'millimeters', 'ha':'hectares', '\"':'inches', 'cm':'centimeters', '/day' : 'per day', 'nm' : 'nanometers', '/s':'per second', 'm2':'square meters', 'km2': 'square kilometers', 'percent': 'percent'}\n",
    "        #Comprehensive list of all measures\n",
    "        replacement = {'\"': 'inches', \"'\": 'feet', 'km/s': 'kilometers per second', 'AU': 'units', 'BAR': 'bars', 'CM': 'centimeters', 'mm': 'millimeters', 'FT': 'feet', 'G': 'grams', \n",
    "     'GAL': 'gallons', 'GB': 'gigabytes', 'GHZ': 'gigahertz', 'HA': 'hectares', 'HP': 'horsepower', 'HZ': 'hertz', 'KM':'kilometers', 'km3': 'cubic kilometers',\n",
    "     'KA':'kilo amperes', 'KB': 'kilobytes', 'KG': 'kilograms', 'KHZ': 'kilohertz', 'KM²': 'square kilometers', 'KT': 'knots', 'KV': 'kilo volts', 'M': 'meters',\n",
    "      'KM2': 'square kilometers','Kw':'kilowatts', 'KWH': 'kilo watt hours', 'LB': 'pounds', 'LBS': 'pounds', 'MA': 'mega amperes', 'MB': 'megabytes',\n",
    "     'KW': 'kilowatts', 'MPH': 'miles per hour', 'MS': 'milliseconds', 'MV': 'milli volts', 'kJ':'kilojoules', 'km/h': 'kilometers per hour',  'V': 'volts', \n",
    "     'M2': 'square meters', 'M3': 'cubic meters', 'MW': 'megawatts', 'M²': 'square meters', 'M³': 'cubic meters', 'OZ': 'ounces',  'MHZ': 'megahertz', 'MI': 'miles',\n",
    "     'MB/S': 'megabytes per second', 'MG': 'milligrams', 'ML': 'milliliters', 'YD': 'yards', 'au': 'units', 'bar': 'bars', 'cm': 'centimeters', 'ft': 'feet', 'g': 'grams', \n",
    "     'gal': 'gallons', 'gb': 'gigabytes', 'ghz': 'gigahertz', 'ha': 'hectares', 'hp': 'horsepower', 'hz': 'hertz', 'kWh': 'kilo watt hours', 'ka': 'kilo amperes', 'kb': 'kilobytes', \n",
    "     'kg': 'kilograms', 'khz': 'kilohertz', 'km': 'kilometers', 'km2': 'square kilometers', 'km²': 'square kilometers', 'kt': 'knots','kv': 'kilo volts', 'kw': 'kilowatts', \n",
    "     'lb': 'pounds', 'lbs': 'pounds', 'm': 'meters', 'm2': 'square meters','m3': 'cubic meters', 'ma': 'mega amperes', 'mb': 'megabytes', 'mb/s': 'megabytes per second', \n",
    "     'mg': 'milligrams', 'mhz': 'megahertz', 'mi': 'miles', 'ml': 'milliliters', 'mph': 'miles per hour','ms': 'milliseconds', 'mv': 'milli volts', 'mw': 'megawatts', 'm²': 'square meters',\n",
    "     'm³': 'cubic meters', 'oz': 'ounces', 'v': 'volts', 'yd': 'yards', 'µg': 'micrograms', 'ΜG': 'micrograms', 'kg/m3': 'kilograms per meter cube'}\n",
    "        result_string = ''\n",
    "        if re.match('.*%$',x[0]):\n",
    "            x = re.sub('%','',x[0])\n",
    "            x = cardinal(x)\n",
    "            result_string = result_string + x + ' percent' \n",
    "        elif re.match('.*\\\"$',x[0]):\n",
    "            x = re.sub('\\\"','',x[0])\n",
    "            x = cardinal(x)\n",
    "            result_string = result_string + x + ' inches' \n",
    "        elif len(x)<2:\n",
    "            return x\n",
    "        elif re.match('.*ft$', x[0]):\n",
    "            x = re.sub('ft','',x[0])\n",
    "            x = cardinal(x)\n",
    "            result_string = result_string + x1 + ' ' + replacement['ft']\n",
    "        elif x[1] in replacement:\n",
    "            x1 = cardinal(x[0])\n",
    "\n",
    "            result_string = result_string + x1 + ' ' + replacement[x[1]] \n",
    "        else:\n",
    "            result_string = x\n",
    "        return(result_string)\n",
    "    except:\n",
    "        return x\n",
    "\n",
    "def letters(x):\n",
    "    try:\n",
    "        x = re.sub('[^a-zA-Z]', '', x)\n",
    "        x = x.lower()\n",
    "        result_string = ''\n",
    "        for i in range(len(x)):\n",
    "            result_string = result_string + x[i] + ' '\n",
    "        return(result_string.strip())  \n",
    "    except:\n",
    "        return x\n",
    "    \n",
    "    \n",
    "def decimal(x):\n",
    "    try:\n",
    "        x = re.sub(',', '', count=10)\n",
    "        x = x.split(' ')\n",
    "        if len(x) == 1:\n",
    "            result_string = cardinal(x[0])\n",
    "        else:\n",
    "            result_string = cardinal(x[0]) + ' ' + x[1]\n",
    "        return result_string   \n",
    "    except:\n",
    "        return x\n",
    "\n",
    "def ordinal(x):\n",
    "    try:\n",
    "        result_string = ''\n",
    "        x = x.replace(',', '')\n",
    "        x = x.replace('[\\.]$', '')\n",
    "        if re.match('^[0-9]+$',x):\n",
    "            x = num2words(int(x), ordinal=True)\n",
    "            return(x.replace('-', ' '))\n",
    "        if re.match('.*V|X|I|L|D',x):\n",
    "            if re.match('.*th|st|nd|rd',x):\n",
    "                x = x[0:len(x)-2]\n",
    "                x = rom_to_int(x)\n",
    "                result_string = re.sub('-', ' ',  num2words(x, ordinal=True))\n",
    "            else:\n",
    "                x = rom_to_int(x)\n",
    "                result_string = 'the '+ re.sub('-', ' ',  num2words(x, ordinal=True))\n",
    "        else:\n",
    "            x = x[0:len(x)-2]\n",
    "            result_string = re.sub('-', ' ',  num2words(float(x), ordinal=True))\n",
    "        return(result_string)  \n",
    "    except:\n",
    "        return x\n",
    "\n",
    "def electronic(x):\n",
    "    try:\n",
    "        replacement = {'.' : 'dot', ':' : 'colon', '/':'slash', '-' : 'dash', '#' : 'hash tag', }\n",
    "        result_string = ''\n",
    "        if re.match('.*[A-Za-z].*', x):\n",
    "            for char in x:\n",
    "                if re.match('[A-Za-z]', char):\n",
    "                    result_string = result_string + letters(char) + ' '\n",
    "                elif char in replacement:\n",
    "                    result_string = result_string + replacement[char] + ' '\n",
    "                elif re.match('[0-9]', char):\n",
    "                    if char == 0:\n",
    "                        result_string = result_string + 'o '\n",
    "                    else:\n",
    "                        number = cardinal(char)\n",
    "                        for n in number:\n",
    "                            result_string = result_string + n + ' ' \n",
    "            return result_string.strip()                \n",
    "        else:\n",
    "            return(x)\n",
    "    except:    \n",
    "        return(x)\n",
    "\n",
    "def address(x):\n",
    "    try:\n",
    "        x = re.sub('[^0-9a-zA-Z]+', '', x)\n",
    "        result_string = ''\n",
    "        for i in range(0,len(x)):\n",
    "            if re.match('[A-Z]|[a-z]',x[i]):\n",
    "                result_string = result_string + plain(x[i]).lower() + ' '\n",
    "            else:\n",
    "                result_string = result_string + cardinal(x[i]) + ' '\n",
    "                \n",
    "        return(result_string.strip())        \n",
    "    except:    \n",
    "        return(x)\n",
    "\n",
    "def telephone(x):\n",
    "    try:\n",
    "        result_string = ''\n",
    "        for i in range(0,len(x)):\n",
    "            if re.match('[0-9]+', x[i]):\n",
    "                result_string = result_string + cardinal(x[i]) + ' '\n",
    "            else:\n",
    "                result_string = result_string + 'sil '\n",
    "        return result_string.strip()    \n",
    "    except:    \n",
    "        return(x)\n",
    "\n",
    "def time(x):\n",
    "    return(x)\n",
    "\n",
    "def money(x):\n",
    "    try:\n",
    "        if re.match('^\\$', x):\n",
    "            x = x.replace('$','')\n",
    "            text = cardinal(x)\n",
    "            x = text + ' dollars'\n",
    "            return x.lower()\n",
    "\n",
    "        elif re.match('^£', x):\n",
    "            x = x.replace('£','')\n",
    "            text = cardinal(x)\n",
    "            x = text+ ' pounds'\n",
    "            return x.lower()   \n",
    "            \n",
    "        elif re.match('^€', x):\n",
    "            x = x.replace('€','')\n",
    "            text = cardinal(x)\n",
    "            x = text+ ' euros'\n",
    "            return x.lower()        \n",
    "    except:    \n",
    "        return(x)\n",
    "\n",
    "def fraction(x):\n",
    "    try:\n",
    "        y = x.split('/')\n",
    "        result_string = ''\n",
    "        y[0] = cardinal(y[0])\n",
    "        y[1] = ordinal(y[1])\n",
    "        if y[1] == 4:\n",
    "            result_string = y[0] + ' quarters'\n",
    "        else:    \n",
    "            result_string = y[0] + ' ' + y[1] + 's'\n",
    "        return(result_string)\n",
    "    except:    \n",
    "        return(x)\n",
    "    \n",
    "    \n",
    "def digit(x): \n",
    "    try:\n",
    "        x = re.sub('[^0-9]', '',x)\n",
    "        result_string = ''\n",
    "        for i in x:\n",
    "            result_string = result_string + cardinal(i) + ' '\n",
    "        result_string = result_string.strip()\n",
    "        return result_string\n",
    "    except:\n",
    "        return(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'hash tag l i f e t i m e b i o p i c s o n e t w o t h r e e dot c o m'"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    " x = '#LifetimeBiopics123.com'\n",
    "electronic(x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "after = []\n",
    "for i in range(test.shape[0]):\n",
    "#    print(test.iloc[i,3])\n",
    "#     print(after)\n",
    "    if test.iloc[i,3] in train_dict.keys():\n",
    "        after.append(train_dict[test.iloc[i,3]])\n",
    "    elif test.iloc[i,3] in p1.keys() and test.iloc[i,3] != '-' and test.iloc[i,3] != '.' and test.iloc[i,3] != '\"' and test.iloc[i,3] != '/' and test.iloc[i,3] != '\\\\' and test.iloc[i,3] != ',':\n",
    "        if p1[test.iloc[i,3]] == '<self>':\n",
    "            after.append(test.iloc[i,3])\n",
    "        else:\n",
    "            after.append(p1[test.iloc[i,3]]) \n",
    "    else:    \n",
    "        class_name = test.iloc[i,4]\n",
    "        if class_name == 'PLAIN':\n",
    "            after.append(plain(test.iloc[i,3]))\n",
    "        elif class_name == 'PUNCT':\n",
    "            after.append(punct(test.iloc[i,3]))\n",
    "        elif class_name  == 'CARDINAL':\n",
    "            after.append(cardinal(test.iloc[i,3]))\n",
    "        elif class_name == 'VERBATIM':\n",
    "            after.append(verbatim(test.iloc[i,3]))\n",
    "        elif class_name == 'DATE':\n",
    "            after.append(date(test.iloc[i,3]))\n",
    "        elif class_name == 'MEASURE':\n",
    "            after.append(measure(test.iloc[i,3]))\n",
    "        elif class_name == 'LETTERS':\n",
    "            after.append(letters(test.iloc[i,3]))\n",
    "        elif class_name == 'DECIMAL':\n",
    "            after.append(decimal(test.iloc[i,3]))\n",
    "        elif class_name == 'ORDINAL':\n",
    "            after.append(ordinal(test.iloc[i,3]))\n",
    "        elif class_name == 'ELECTRONIC':\n",
    "            after.append(electronic(test.iloc[i,3]))\n",
    "        elif class_name == 'ADDRESS':\n",
    "            after.append(address(test.iloc[i,3]))\n",
    "        elif class_name == 'DIGIT':\n",
    "            after.append(digit(test.iloc[i,3]))\n",
    "        elif class_name == 'MONEY':\n",
    "            after.append(money(test.iloc[i,3]))\n",
    "        elif class_name == 'TIME':\n",
    "            after.append(time(test.iloc[i,3]))\n",
    "        elif class_name == 'TELEPHONE':\n",
    "            after.append(telephone(test.iloc[i,3]))\n",
    "        elif class_name == 'FRACTION':\n",
    "            after.append(fraction(test.iloc[i,3]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Last',\n",
       " 'modified',\n",
       " 'the thirty first of march twenty sixteen',\n",
       " '.',\n",
       " \"There's\",\n",
       " 'More',\n",
       " 'to',\n",
       " 'Clear',\n",
       " 'Channel',\n",
       " 'Than',\n",
       " \"'\",\n",
       " 'The',\n",
       " 'Larry',\n",
       " 'King',\n",
       " 'Show',\n",
       " \"'\",\n",
       " '.',\n",
       " 'Roberto',\n",
       " 'Chiti',\n",
       " ';',\n",
       " 'Roberto',\n",
       " 'Poppi',\n",
       " ';',\n",
       " 'Enrico',\n",
       " 'Lancia',\n",
       " '.',\n",
       " 'The',\n",
       " 'party',\n",
       " 'applied',\n",
       " 'to',\n",
       " 'register',\n",
       " 'this',\n",
       " 'with',\n",
       " 'the',\n",
       " 'Electoral',\n",
       " 'Commission',\n",
       " 'in',\n",
       " 'april twenty seventeen',\n",
       " 'and',\n",
       " 'it',\n",
       " 'was',\n",
       " 'approved',\n",
       " 'in',\n",
       " 'may twenty seventeen',\n",
       " '.',\n",
       " '21 february 2017',\n",
       " '.',\n",
       " '\"',\n",
       " 'Passport',\n",
       " 'and',\n",
       " 'visa',\n",
       " 'requirements',\n",
       " '\"',\n",
       " '.',\n",
       " '\"',\n",
       " 'Main',\n",
       " 'Street',\n",
       " 'Electrical',\n",
       " 'Parade',\n",
       " 'Extended',\n",
       " 'by',\n",
       " 'Popular',\n",
       " 'Demand',\n",
       " 'at',\n",
       " 'Disneyland',\n",
       " 'Park',\n",
       " '\"',\n",
       " '.',\n",
       " 'International',\n",
       " 'Air',\n",
       " 'Transport',\n",
       " 'Association',\n",
       " '(',\n",
       " 'i a t a',\n",
       " ')',\n",
       " 'through',\n",
       " 'Olympic',\n",
       " 'Air',\n",
       " '.',\n",
       " 'He',\n",
       " 'was',\n",
       " 'buried',\n",
       " 'in',\n",
       " 'the',\n",
       " 'chancel',\n",
       " 'of',\n",
       " 'his',\n",
       " 'former',\n",
       " 'church',\n",
       " 'in',\n",
       " 'Raleigh',\n",
       " ',',\n",
       " 'which',\n",
       " 'was',\n",
       " 'rebuilt',\n",
       " 'several',\n",
       " 'years',\n",
       " 'later',\n",
       " '.',\n",
       " 'Receptionist',\n",
       " 'Noel',\n",
       " 'Garcia',\n",
       " ',',\n",
       " 'portrayed',\n",
       " 'by',\n",
       " 'Tony',\n",
       " 'Marshall',\n",
       " ',',\n",
       " 'guest',\n",
       " 'starred',\n",
       " 'in',\n",
       " 'episode',\n",
       " 'twenty',\n",
       " 'three',\n",
       " '.',\n",
       " 'The',\n",
       " 'Christian',\n",
       " 'Journal',\n",
       " ',',\n",
       " 'and',\n",
       " 'Literary',\n",
       " 'Register',\n",
       " '.',\n",
       " 'The',\n",
       " 'station',\n",
       " 'has',\n",
       " 'a',\n",
       " 'seventeen',\n",
       " '-',\n",
       " 'space',\n",
       " 'parking',\n",
       " 'lot',\n",
       " '.',\n",
       " 'V',\n",
       " '.',\n",
       " 'Annales',\n",
       " 'de',\n",
       " \"l'Institut\",\n",
       " 'océanographique',\n",
       " '.',\n",
       " 'Stenoptilia',\n",
       " 'exclamationis',\n",
       " 'is',\n",
       " 'a',\n",
       " 'moth',\n",
       " 'of',\n",
       " 'the',\n",
       " 'family',\n",
       " 'Pterophoridae',\n",
       " '.',\n",
       " 'Retrieved',\n",
       " '4 march 2017',\n",
       " '.',\n",
       " 'All',\n",
       " 'rounded',\n",
       " 'midfielder',\n",
       " ',',\n",
       " 'captain',\n",
       " 'of',\n",
       " 'The',\n",
       " 'Name',\n",
       " 'of',\n",
       " 'Devils',\n",
       " '.',\n",
       " 'Retrieved',\n",
       " 'march twenty eighth twenty seventeen',\n",
       " '.',\n",
       " 'The',\n",
       " 'thorax',\n",
       " 'is',\n",
       " 'gray',\n",
       " ',',\n",
       " 'with',\n",
       " 'a',\n",
       " 'brown',\n",
       " 'spot',\n",
       " 'on',\n",
       " 'the',\n",
       " 'top',\n",
       " ',',\n",
       " 'and',\n",
       " 'the',\n",
       " 'antennae',\n",
       " 'are',\n",
       " 'brownish',\n",
       " 'gray',\n",
       " '.',\n",
       " 'Bold',\n",
       " 'Brown',\n",
       " 'Women',\n",
       " '.',\n",
       " 'Because',\n",
       " 'of',\n",
       " 'their',\n",
       " 'highly',\n",
       " 'tunable',\n",
       " 'properties',\n",
       " ',',\n",
       " \"q d's\",\n",
       " 'are',\n",
       " 'of',\n",
       " 'wide',\n",
       " 'interest',\n",
       " '.',\n",
       " 'Retrieved',\n",
       " 'april eighteenth twenty seventeen',\n",
       " '.',\n",
       " 'He',\n",
       " 'was',\n",
       " 'named',\n",
       " 'a',\n",
       " 'c f l',\n",
       " 'East',\n",
       " 'all-star',\n",
       " 'and',\n",
       " 'a',\n",
       " 'c f l',\n",
       " 'a l l s t a r',\n",
       " '.',\n",
       " 'King',\n",
       " 'was',\n",
       " 'very',\n",
       " 'interested',\n",
       " 'in',\n",
       " 'this',\n",
       " 'racially',\n",
       " 'mixed',\n",
       " 'church',\n",
       " 'and',\n",
       " 'made',\n",
       " 'arrangements',\n",
       " 'to',\n",
       " 'visit',\n",
       " 'it',\n",
       " 'the',\n",
       " 'next',\n",
       " 'day',\n",
       " '.',\n",
       " 'Other',\n",
       " 'theories',\n",
       " 'include',\n",
       " 'origins',\n",
       " 'in',\n",
       " 'the',\n",
       " 'Arabian',\n",
       " 'Peninsula',\n",
       " 'or',\n",
       " 'North',\n",
       " 'Africa',\n",
       " '.',\n",
       " 'march ninth twenty seventeen',\n",
       " '.',\n",
       " 'In',\n",
       " 'twenty fifteen',\n",
       " ',',\n",
       " 'Proneeta',\n",
       " 'Swargiary',\n",
       " 'from',\n",
       " 'Punit',\n",
       " 'k e',\n",
       " 'Panthers',\n",
       " 'won',\n",
       " 'the',\n",
       " 'd i d',\n",
       " 'five',\n",
       " 'title',\n",
       " '.',\n",
       " 'It',\n",
       " 'was',\n",
       " 'owned',\n",
       " 'by',\n",
       " 'Alberto',\n",
       " 'Barragan',\n",
       " 'Degollado',\n",
       " 'and',\n",
       " 'sold',\n",
       " 'to',\n",
       " 'José',\n",
       " 'Raul',\n",
       " 'Nava',\n",
       " 'Becerra',\n",
       " 'in',\n",
       " 'nineteen eighty two',\n",
       " '.',\n",
       " 'They',\n",
       " 'hypothesized',\n",
       " 'that',\n",
       " 'the',\n",
       " 'geologic',\n",
       " 'features',\n",
       " 'associated',\n",
       " 'with',\n",
       " 'the',\n",
       " 'artifacts',\n",
       " 'could',\n",
       " 'be',\n",
       " 'used',\n",
       " 'to',\n",
       " 'date',\n",
       " 'the',\n",
       " 'period',\n",
       " 'of',\n",
       " 'human',\n",
       " 'habitation',\n",
       " '.',\n",
       " '\"',\n",
       " 'plug-in',\n",
       " 'grant',\n",
       " 'eligible',\n",
       " 'vehicles',\n",
       " 'licensed',\n",
       " '\"',\n",
       " '.',\n",
       " 'During',\n",
       " 'this',\n",
       " 'time',\n",
       " 'she',\n",
       " 'also',\n",
       " 'wrote',\n",
       " 'a',\n",
       " 'memoir',\n",
       " 'about',\n",
       " 'homesteading',\n",
       " 'in',\n",
       " 'Twentynine',\n",
       " 'Palms',\n",
       " ',',\n",
       " 'The',\n",
       " 'Desert',\n",
       " 'was',\n",
       " 'Home',\n",
       " '.',\n",
       " 'Retrieved',\n",
       " 'on',\n",
       " 'the seventeenth of june twenty sixteen',\n",
       " '.',\n",
       " 'Grubb',\n",
       " 'and',\n",
       " 'Depledge',\n",
       " 'two thousand one',\n",
       " ',',\n",
       " 'p',\n",
       " '.',\n",
       " 'two hundred sixty nine',\n",
       " '\"',\n",
       " 'Article',\n",
       " 'two',\n",
       " '\"',\n",
       " '.',\n",
       " 'Retrieved',\n",
       " 'the fifteenth of august twenty sixteen',\n",
       " '.',\n",
       " 'College',\n",
       " 'Board',\n",
       " ',',\n",
       " 'Class',\n",
       " 'of',\n",
       " 'twenty sixteen',\n",
       " 'SAT',\n",
       " 'Participation',\n",
       " 'and',\n",
       " 'Performance',\n",
       " 'Data',\n",
       " ',',\n",
       " 'twenty seventeen',\n",
       " 'Pennsylvania',\n",
       " 'Department',\n",
       " 'of',\n",
       " 'Education',\n",
       " '(',\n",
       " 'twenty fifteen',\n",
       " ')',\n",
       " '.',\n",
       " 'Textiles',\n",
       " 'can',\n",
       " 'make',\n",
       " 'up',\n",
       " 'many',\n",
       " 'different',\n",
       " 'objects',\n",
       " 'from',\n",
       " 'cushions',\n",
       " 'to',\n",
       " 'dresses',\n",
       " '.',\n",
       " 'He',\n",
       " 'was',\n",
       " 'Lucky',\n",
       " \"Thompson's\",\n",
       " 'drummer',\n",
       " 'during',\n",
       " \"Thompson's\",\n",
       " 'nineteen sixty nine',\n",
       " 'Swiss',\n",
       " 'tour',\n",
       " '.',\n",
       " 'los',\n",
       " 'angeles',\n",
       " 'california',\n",
       " ':',\n",
       " 'Southwest',\n",
       " 'Museum',\n",
       " '(',\n",
       " 'nine',\n",
       " ')',\n",
       " '.',\n",
       " 'Poria',\n",
       " '—',\n",
       " 'Labor',\n",
       " 'Village',\n",
       " ')',\n",
       " 'is',\n",
       " 'a',\n",
       " 'community',\n",
       " 'settlement',\n",
       " 'in',\n",
       " 'northern',\n",
       " 'Israel',\n",
       " '.',\n",
       " '\"',\n",
       " 'nine',\n",
       " 'Land',\n",
       " 'Banking',\n",
       " 'Companies',\n",
       " 'Wound',\n",
       " 'Up',\n",
       " '\"',\n",
       " '.',\n",
       " 'Du',\n",
       " 'Tems',\n",
       " ',',\n",
       " 'i v',\n",
       " ',',\n",
       " 'p',\n",
       " '.',\n",
       " 'five hundred eighty two',\n",
       " '.',\n",
       " 'Reflections',\n",
       " 'on',\n",
       " 'Little',\n",
       " 'Rock',\n",
       " '(',\n",
       " 'nineteen fifty nine',\n",
       " ')',\n",
       " '.',\n",
       " 'National',\n",
       " 'Park',\n",
       " 'Service',\n",
       " '(',\n",
       " 'the thirteenth of march two thousand nine',\n",
       " ')',\n",
       " '.',\n",
       " 'fourth',\n",
       " 'edition',\n",
       " ',',\n",
       " 'revisied',\n",
       " '.',\n",
       " 'Rubber',\n",
       " 'Chemistry',\n",
       " 'and',\n",
       " 'Technology',\n",
       " '.',\n",
       " 'It',\n",
       " 'is',\n",
       " 'licensed',\n",
       " 'to',\n",
       " 'the',\n",
       " 'city',\n",
       " 'of',\n",
       " 'Puerto',\n",
       " 'Nuevo',\n",
       " ',',\n",
       " 'and',\n",
       " 'shares',\n",
       " 'tower',\n",
       " 'space',\n",
       " 'with',\n",
       " 'sister',\n",
       " 'station',\n",
       " 'x e s d d',\n",
       " 'AM',\n",
       " '.',\n",
       " 'february fifteenth twenty seventeen',\n",
       " '.',\n",
       " 'Zherebetskyy',\n",
       " 'd',\n",
       " ';',\n",
       " 'Scheele',\n",
       " 'm',\n",
       " ';',\n",
       " 'Zhang',\n",
       " 'y',\n",
       " ';',\n",
       " 'Bronstein',\n",
       " 'n',\n",
       " ';',\n",
       " 'Thompson',\n",
       " 'c',\n",
       " ';',\n",
       " 'Britt',\n",
       " 'd',\n",
       " ';',\n",
       " 'Salmeron',\n",
       " 'm',\n",
       " ';',\n",
       " 'Alivisatos',\n",
       " 'p',\n",
       " ';',\n",
       " 'Wang',\n",
       " 'l w',\n",
       " '(',\n",
       " 'twenty fourteen',\n",
       " ')',\n",
       " '.',\n",
       " 'Int',\n",
       " 'J',\n",
       " 'Biol',\n",
       " 'Markers',\n",
       " '.',\n",
       " '\"',\n",
       " 'Greenwood',\n",
       " 'Elementary',\n",
       " 'School',\n",
       " 'Fast',\n",
       " 'Facts',\n",
       " 'twenty sixteen',\n",
       " '\"',\n",
       " '.',\n",
       " 'In',\n",
       " 'may twenty fourteen',\n",
       " ',',\n",
       " 'K',\n",
       " 'two',\n",
       " 'campaign',\n",
       " 'fields',\n",
       " 'zero',\n",
       " 'to',\n",
       " 'thirteen',\n",
       " 'were',\n",
       " 'announced',\n",
       " 'and',\n",
       " 'described',\n",
       " 'in',\n",
       " 'detail',\n",
       " '.',\n",
       " 'In',\n",
       " 'the',\n",
       " 'late',\n",
       " 'nineteen sixties',\n",
       " ',',\n",
       " 'she',\n",
       " 'and',\n",
       " 'Edmund',\n",
       " 'divorced',\n",
       " 'and',\n",
       " 'Szekely',\n",
       " 'took',\n",
       " 'over',\n",
       " 'the',\n",
       " 'operation',\n",
       " 'of',\n",
       " 'the',\n",
       " 'Rancho',\n",
       " 'la',\n",
       " 'Puerta',\n",
       " '.',\n",
       " '\"',\n",
       " 'We',\n",
       " 'look',\n",
       " 'forward',\n",
       " 'to',\n",
       " 'informing',\n",
       " 'the',\n",
       " 'u s',\n",
       " 'administration',\n",
       " 'about',\n",
       " 'Swedish',\n",
       " 'immigration',\n",
       " 'and',\n",
       " 'integration',\n",
       " 'policies',\n",
       " '\"',\n",
       " '.',\n",
       " 'Stabilizing',\n",
       " 'the',\n",
       " 'concentration',\n",
       " 'of',\n",
       " 'c o',\n",
       " 'two',\n",
       " 'in',\n",
       " 'the',\n",
       " 'atmosphere',\n",
       " 'would',\n",
       " 'ultimately',\n",
       " 'require',\n",
       " 'the',\n",
       " 'effective',\n",
       " 'elimination',\n",
       " 'of',\n",
       " 'anthropogenic',\n",
       " 'c o',\n",
       " 'two',\n",
       " 'emissions',\n",
       " '.',\n",
       " 'Peng',\n",
       " ',',\n",
       " 'Guicheng',\n",
       " ';',\n",
       " 'Lin',\n",
       " ',',\n",
       " 'Maohuan',\n",
       " ';',\n",
       " 'Zhang',\n",
       " ',',\n",
       " 'Kun',\n",
       " ';',\n",
       " 'Chen',\n",
       " ',',\n",
       " 'Jie',\n",
       " ';',\n",
       " 'Wang',\n",
       " ',',\n",
       " 'Yifang',\n",
       " ';',\n",
       " 'Yang',\n",
       " ',',\n",
       " 'Yu',\n",
       " ';',\n",
       " 'Wang',\n",
       " ',',\n",
       " 'Jingfeng',\n",
       " ';',\n",
       " 'Huang',\n",
       " ',',\n",
       " 'Hui',\n",
       " '(',\n",
       " 'the nineteenth of june twenty thirteen',\n",
       " ')',\n",
       " '.',\n",
       " 'What',\n",
       " 'starts',\n",
       " 'off',\n",
       " 'as',\n",
       " 'a',\n",
       " 'morbid',\n",
       " 'curiosity',\n",
       " 'for',\n",
       " 'the',\n",
       " 'local',\n",
       " 'youth',\n",
       " 'slowly',\n",
       " 'begins',\n",
       " 'to',\n",
       " 'spoil',\n",
       " 'away',\n",
       " 'at',\n",
       " 'their',\n",
       " 'lives',\n",
       " '.',\n",
       " 'After',\n",
       " 'an',\n",
       " 'encounter',\n",
       " 'with',\n",
       " 'death',\n",
       " ',',\n",
       " 'she',\n",
       " 'will',\n",
       " 'do',\n",
       " 'anything',\n",
       " 'to',\n",
       " 'make',\n",
       " 'sure',\n",
       " \"she's\",\n",
       " 'alive',\n",
       " '.',\n",
       " 'Retrieved',\n",
       " 'the tenth of june twenty sixteen',\n",
       " '.',\n",
       " 'Ghebrehiwet',\n",
       " 'B',\n",
       " ',',\n",
       " 'Peerschke',\n",
       " 'EI',\n",
       " ',',\n",
       " 'Hong',\n",
       " 'Y',\n",
       " ',',\n",
       " 'Munoz',\n",
       " 'P',\n",
       " ',',\n",
       " 'Gorevic',\n",
       " 'p d',\n",
       " '(',\n",
       " 'june nineteen ninety two',\n",
       " ')',\n",
       " '.',\n",
       " '\"',\n",
       " 'McLean',\n",
       " 'Business',\n",
       " 'Managers',\n",
       " 'and',\n",
       " 'Strayer',\n",
       " 'University',\n",
       " 'Official',\n",
       " 'Convicted',\n",
       " ',',\n",
       " 'Sentenced',\n",
       " 'for',\n",
       " 'Large',\n",
       " 'Scale',\n",
       " 'Immigration',\n",
       " 'Fraud',\n",
       " '\"',\n",
       " '.',\n",
       " 'There',\n",
       " 'were',\n",
       " 'one thousand two hundred sixty eight',\n",
       " 'housing',\n",
       " 'units',\n",
       " 'at',\n",
       " 'an',\n",
       " 'average',\n",
       " 'density',\n",
       " 'of',\n",
       " '76.2/sq mi',\n",
       " '(',\n",
       " 'twenty nine point four per square kilometers',\n",
       " ')',\n",
       " '.',\n",
       " 'Dieudonné',\n",
       " ',',\n",
       " 'Florence',\n",
       " 'Lucinda',\n",
       " 'Carpenter',\n",
       " '.',\n",
       " 'Their',\n",
       " 'most',\n",
       " 'recent',\n",
       " 'split',\n",
       " 'was',\n",
       " 'in',\n",
       " 'two thousand six',\n",
       " ',',\n",
       " 'but',\n",
       " 'they',\n",
       " 'r e f o r m e d',\n",
       " 'in',\n",
       " 'two thousand nine',\n",
       " ',',\n",
       " 'and',\n",
       " 'continue',\n",
       " 'performing',\n",
       " 'today',\n",
       " '.',\n",
       " 'Hammond',\n",
       " ',',\n",
       " 'Graeme',\n",
       " '(',\n",
       " 'the first of august two thousand four',\n",
       " ')',\n",
       " '.',\n",
       " 'Documentary',\n",
       " 'about',\n",
       " 'and',\n",
       " 'including',\n",
       " 'Salgado',\n",
       " ',',\n",
       " 'directed',\n",
       " 'by',\n",
       " 'w i m',\n",
       " 'Wenders',\n",
       " 'and',\n",
       " 'his',\n",
       " 'son',\n",
       " 'Juliano',\n",
       " 'Ribeiro',\n",
       " 'Salgado',\n",
       " '.',\n",
       " 'Fernea',\n",
       " 'became',\n",
       " 'a',\n",
       " 'senior',\n",
       " 'lecturer',\n",
       " 'in',\n",
       " 'nineteen seventy five',\n",
       " 'for',\n",
       " 'the',\n",
       " 'University',\n",
       " 'of',\n",
       " 'Texas',\n",
       " 'and',\n",
       " 'eventually',\n",
       " 'a',\n",
       " 'full-time',\n",
       " 'professor',\n",
       " 'in',\n",
       " 'nineteen ninety',\n",
       " '.',\n",
       " 'Retrieved',\n",
       " 'the sixth of december twenty sixteen',\n",
       " '.',\n",
       " 'the twenty fifth of june twenty sixteen',\n",
       " '.',\n",
       " 'Retrieved',\n",
       " 'february fourteenth twenty seventeen',\n",
       " '.',\n",
       " 'Herculaneum',\n",
       " 'also',\n",
       " 'had',\n",
       " 'a',\n",
       " 'Baptist',\n",
       " 'church',\n",
       " 'from',\n",
       " 'its',\n",
       " 'founding',\n",
       " 'in',\n",
       " 'nineteen o eight',\n",
       " 'until',\n",
       " 'its',\n",
       " \"building's\",\n",
       " 'destruction',\n",
       " 'by',\n",
       " 'fire',\n",
       " 'in',\n",
       " 'nineteen ninety one',\n",
       " '.',\n",
       " 'doctor',\n",
       " 'Abhay',\n",
       " 'r',\n",
       " 'Vasavada',\n",
       " 'started',\n",
       " 'Raghudeep',\n",
       " 'Eye',\n",
       " 'Clinic',\n",
       " '(',\n",
       " 'REH',\n",
       " ')',\n",
       " '-',\n",
       " 'as',\n",
       " 'a',\n",
       " 'cataract',\n",
       " 'speciality',\n",
       " 'center',\n",
       " 'in',\n",
       " 'nineteen',\n",
       " 'eighty four',\n",
       " 'Ahmedabad',\n",
       " ',',\n",
       " 'India',\n",
       " '.',\n",
       " 'He',\n",
       " 'made',\n",
       " 'his',\n",
       " 'formal',\n",
       " 'entry',\n",
       " 'on',\n",
       " '3 june 1712',\n",
       " '.',\n",
       " 'Valli',\n",
       " 'gets',\n",
       " 'pregnant',\n",
       " '.',\n",
       " 'Richard',\n",
       " 'Wright',\n",
       " 'Willett',\n",
       " '.',\n",
       " 'Cell',\n",
       " 'and',\n",
       " 'Tissue',\n",
       " 'Research',\n",
       " '.',\n",
       " 'Clayton',\n",
       " 'Wood',\n",
       " 'is',\n",
       " 'the',\n",
       " 'training',\n",
       " 'ground',\n",
       " 'and',\n",
       " 'academy',\n",
       " 'of',\n",
       " 'Stoke',\n",
       " 'City',\n",
       " '.',\n",
       " 'Chennai',\n",
       " ':',\n",
       " 'Poet',\n",
       " 'Publications',\n",
       " '.',\n",
       " 'This',\n",
       " 'count',\n",
       " 'includes',\n",
       " 'playing',\n",
       " 'members',\n",
       " ',',\n",
       " 'auxiliary',\n",
       " ',',\n",
       " 'and',\n",
       " 'command',\n",
       " 'personnel',\n",
       " 'drum',\n",
       " 'majors',\n",
       " '.',\n",
       " 'Rochefort',\n",
       " ':',\n",
       " 'Jean',\n",
       " ',',\n",
       " 'p',\n",
       " '.',\n",
       " 'two hundred twenty five',\n",
       " '.',\n",
       " 'Retrieved',\n",
       " 'the fifth of may twenty sixteen',\n",
       " '.',\n",
       " 'Larkin',\n",
       " 'later',\n",
       " 'reflected',\n",
       " 'that',\n",
       " 'You',\n",
       " 'Am',\n",
       " 'I',\n",
       " '\"',\n",
       " 'were',\n",
       " 'instrumental',\n",
       " 'for',\n",
       " 'us',\n",
       " 'in',\n",
       " 'many',\n",
       " 'ways',\n",
       " '.',\n",
       " 'The',\n",
       " 'main',\n",
       " 'ragas',\n",
       " '.',\n",
       " 'However',\n",
       " ',',\n",
       " 'subsequent',\n",
       " 'X',\n",
       " '-',\n",
       " 'rays',\n",
       " 'have',\n",
       " 'still',\n",
       " 'produced',\n",
       " 'nothing',\n",
       " 'legible',\n",
       " '.',\n",
       " 'Eighteen',\n",
       " 'pupils',\n",
       " 'were',\n",
       " 'killed',\n",
       " ',',\n",
       " 'of',\n",
       " 'whom',\n",
       " 'sixteen',\n",
       " 'were',\n",
       " 'aged',\n",
       " 'from',\n",
       " 'four',\n",
       " 'to',\n",
       " '6 years',\n",
       " 'old',\n",
       " '.',\n",
       " 'Retrieved',\n",
       " '14 june 2017',\n",
       " '.',\n",
       " '\"',\n",
       " 'a e t',\n",
       " 'challenged',\n",
       " 'by',\n",
       " 'action',\n",
       " 'group',\n",
       " 'over',\n",
       " 'planned',\n",
       " 'closure',\n",
       " 'of',\n",
       " 'Weston',\n",
       " 'Academy',\n",
       " '\"',\n",
       " '.',\n",
       " 'This',\n",
       " 'increase',\n",
       " 'in',\n",
       " 'prevalence',\n",
       " 'of',\n",
       " 'chronic',\n",
       " 'diseases',\n",
       " 'has',\n",
       " 'been',\n",
       " 'attributed',\n",
       " 'to',\n",
       " 'lifestyle',\n",
       " 'changes',\n",
       " 'and',\n",
       " 'increased',\n",
       " 'urbanization',\n",
       " '.',\n",
       " 'Retrieved',\n",
       " '7 february 2017',\n",
       " '.',\n",
       " 'Always',\n",
       " 'one',\n",
       " ...]"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "after"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>sentence_id</th>\n",
       "      <th>token_id</th>\n",
       "      <th>before</th>\n",
       "      <th>class</th>\n",
       "      <th>after</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>Last</td>\n",
       "      <td>PLAIN</td>\n",
       "      <td>Last</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>modified</td>\n",
       "      <td>PLAIN</td>\n",
       "      <td>modified</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>2016-03-31</td>\n",
       "      <td>DATE</td>\n",
       "      <td>the thirty first of march twenty sixteen</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>.</td>\n",
       "      <td>PUNCT</td>\n",
       "      <td>.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>There's</td>\n",
       "      <td>PLAIN</td>\n",
       "      <td>There's</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0  sentence_id  token_id      before  class  \\\n",
       "0           0            0         0        Last  PLAIN   \n",
       "1           1            0         1    modified  PLAIN   \n",
       "2           2            0         2  2016-03-31   DATE   \n",
       "3           3            0         3           .  PUNCT   \n",
       "4           4            1         0     There's  PLAIN   \n",
       "\n",
       "                                      after  \n",
       "0                                      Last  \n",
       "1                                  modified  \n",
       "2  the thirty first of march twenty sixteen  \n",
       "3                                         .  \n",
       "4                                   There's  "
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test['after'] = after\n",
    "test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              id                                     after\n",
      "0            0_0                                      Last\n",
      "1            0_1                                  modified\n",
      "2            0_2  the thirty first of march twenty sixteen\n",
      "3            0_3                                         .\n",
      "4            1_0                                   There's\n",
      "5            1_1                                      More\n",
      "6            1_2                                        to\n",
      "7            1_3                                     Clear\n",
      "8            1_4                                   Channel\n",
      "9            1_5                                      Than\n",
      "10           1_6                                         '\n",
      "11           1_7                                       The\n",
      "12           1_8                                     Larry\n",
      "13           1_9                                      King\n",
      "14          1_10                                      Show\n",
      "15          1_11                                         '\n",
      "16          1_12                                         .\n",
      "17           2_0                                   Roberto\n",
      "18           2_1                                     Chiti\n",
      "19           2_2                                         ;\n",
      "20           2_3                                   Roberto\n",
      "21           2_4                                     Poppi\n",
      "22           2_5                                         ;\n",
      "23           2_6                                    Enrico\n",
      "24           2_7                                    Lancia\n",
      "25           2_8                                         .\n",
      "26           3_0                                       The\n",
      "27           3_1                                     party\n",
      "28           3_2                                   applied\n",
      "29           3_3                                        to\n",
      "...          ...                                       ...\n",
      "956016   69997_1                                   Belippo\n",
      "956017   69997_2                                   anguina\n",
      "956018   69997_3                                     Simon\n",
      "956019   69997_4                              nineteen ten\n",
      "956020   69997_5                                         \"\n",
      "956021   69997_6                                         .\n",
      "956022   69998_0                                        At\n",
      "956023   69998_1                                       the\n",
      "956024   69998_2                                      time\n",
      "956025   69998_3                                   Griffin\n",
      "956026   69998_4                                       was\n",
      "956027   69998_5                                   working\n",
      "956028   69998_6                                       for\n",
      "956029   69998_7                                       the\n",
      "956030   69998_8                                     Irish\n",
      "956031   69998_9                                   Revenue\n",
      "956032  69998_10                             Commissioners\n",
      "956033  69998_11                                     based\n",
      "956034  69998_12                                        at\n",
      "956035  69998_13                                    Dublin\n",
      "956036  69998_14                                    Castle\n",
      "956037  69998_15                                         .\n",
      "956038   69999_0                                         \"\n",
      "956039   69999_1                                        EA\n",
      "956040   69999_2                                  releases\n",
      "956041   69999_3                                      FIFA\n",
      "956042   69999_4                                soundtrack\n",
      "956043   69999_5                                      list\n",
      "956044   69999_6                                         \"\n",
      "956045   69999_7                                         .\n",
      "\n",
      "[956046 rows x 2 columns]\n"
     ]
    }
   ],
   "source": [
    "test['id'] = test.sentence_id.astype(str) + '_' + test.token_id.astype(str)\n",
    "\n",
    "print(test[['id', 'after']])\n",
    "test[['id', 'after']].to_csv('output6.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Predictions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "# @Time    : 2017/9/30 8:53\n",
    "# @Author  : LiYun\n",
    "# @File    : main_v2.py\n",
    "'''description:\n",
    "this method is a simple extention of BingQing Wei's XGboost With Context Label Data (ACC: 99.637%)\n",
    "it's accuracy is 99.81% when 10% of the training data is used as validtion data\n",
    "and finally, the whole data is used for training\n",
    "'''\n",
    "import os\n",
    "import gc\n",
    "import pickle\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import xgboost as xgb\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_classify_train_data(np_file,csv_file):\n",
    "    if os.path.exists(np_file) == True:\n",
    "       temp = np.load(np_file)\n",
    "       return temp['x_train'],temp['y_train'],temp['label']\n",
    "    else:\n",
    "        num_features = 9 #每个 word 取前 5 后 4 个字符来编码\n",
    "        train=pd.read_csv(csv_file)\n",
    "        tmp=pd.factorize(train['class'])\n",
    "        y_train,label=tmp[0].astype(np.int8),tmp[1].values\n",
    "        num_train=len(y_train)\n",
    "        train['before']=train['before'].astype(np.str)\n",
    "        x_train=np.zeros([num_train,num_features],np.int8)\n",
    "        feature=np.zeros([num_train,7],np.int8)# 人工提取的特征\n",
    "        list1=('a','e','i','o','u')# 元音\n",
    "        list2=('+','-','*','//','%')# 数学运算符\n",
    "        for word,row in zip(train['before'].values,range(num_train)):\n",
    "            if(len(word)>=num_features):\n",
    "                for c,col in zip(word[:5],range(5)):\n",
    "                    x_train[row,col]=ord(c)\n",
    "                for c,col in zip(word[-4:],range(5,9)):\n",
    "                    x_train[row,col]=ord(c)\n",
    "            else:\n",
    "                for c,col in zip(word,range(num_features)):\n",
    "                    x_train[row,col]=ord(c)\n",
    "            feature[row, 3] =len(word) # 统计字符串的长度\n",
    "            dotflag=0\n",
    "            for c in word:\n",
    "                if c.isdigit():feature[row,0]+=1# 统计数字的个数\n",
    "                if c.isupper():feature[row,1]+=1# 统计大写字母的个数\n",
    "                if c.isalnum()!=True:feature[row,2]+=1# 统计非字母和数字的个数\n",
    "                if c in list1:feature[row,4]+=1# 统计元音的个数\n",
    "                if c=='.': dotflag=1\n",
    "                elif dotflag==1:#  . 后面跟字母置 1 ，数字置 2，其他置 3\n",
    "                    dotflag = 0\n",
    "                    if c.isdigit():feature[row,5]+=10\n",
    "                    elif c.isalpha():feature[row,5]+=100\n",
    "                    else:feature[row,5]+=1000\n",
    "                if c in list2:feature[row,6]+=1# 统计数学运算符的个数\n",
    "\n",
    "        # 掐头去尾，结合上文 2 单词，下文 1 个单词\n",
    "        num_train-=3\n",
    "        y_train=y_train[2:-1]\n",
    "        x_train=np.concatenate((x_train[:-3],x_train[1:-2],x_train[2:-1],x_train[3:],feature[2:-1]),axis=1)\n",
    "        np.savez(np_file,x_train=x_train, y_train=y_train, label=label)\n",
    "        return x_train, y_train, label\n",
    "\n",
    "def get_classify_test_data(np_file,csv_file):\n",
    "    test=pd.read_csv(csv_file)\n",
    "    if os.path.exists(np_file) == True:\n",
    "       temp = np.load(np_file)\n",
    "       x_test=temp['x_test']\n",
    "    else:\n",
    "        num_features = 9 #每个 word 取前 5 后 4 个字符来编码\n",
    "        human_feature=7 #人工提取7个特征\n",
    "        num_test=len(test)\n",
    "        test['before']=test['before'].astype(np.str)\n",
    "        x_test=np.zeros([num_test,num_features],np.int8)\n",
    "        feature=np.zeros([num_test,human_feature],np.int8)# 人工提取的特征\n",
    "        list1=('a','e','i','o','u')# 元音\n",
    "        list2=('+','-','*','//','%')# 数学运算符\n",
    "        for word,row in zip(test['before'].values,range(num_test)):\n",
    "            if(len(word)>=num_features):\n",
    "                for c,col in zip(word[:5],range(5)):\n",
    "                    x_test[row,col]=ord(c)\n",
    "                for c,col in zip(word[-4:],range(5,9)):\n",
    "                    x_test[row,col]=ord(c)\n",
    "            else:\n",
    "                for c,col in zip(word,range(num_features)):\n",
    "                    x_test[row,col]=ord(c)\n",
    "            feature[row, 3] =len(word) # 统计字符串的长度\n",
    "            dotflag=0\n",
    "            for c in word:\n",
    "                if c.isdigit():feature[row,0]+=1# 统计数字的个数\n",
    "                if c.isupper():feature[row,1]+=1# 统计大写字母的个数\n",
    "                if c.isalnum()!=True:feature[row,2]+=1# 统计非字母和数字的个数\n",
    "                if c in list1:feature[row,4]+=1# 统计元音的个数\n",
    "                if c=='.': dotflag=1\n",
    "                elif dotflag==1:#  . 后面跟字母置 1 ，数字置 2，其他置 3\n",
    "                    dotflag = 0\n",
    "                    if c.isdigit():feature[row,5]+=10\n",
    "                    elif c.isalpha():feature[row,5]+=100\n",
    "                    else:feature[row,5]+=1000\n",
    "                if c in list2:feature[row,6]+=1# 统计数学运算符的个数\n",
    "\n",
    "        # 开头补上2个单词,结尾补上1个单词，结合上文 2 单词，下文 1 个单词\n",
    "        x_test = np.concatenate((np.zeros([2,num_features],np.int8),x_test,np.zeros([1,num_features],np.int8)),axis=0)\n",
    "        feature = np.concatenate((np.zeros([2,human_feature],np.int8),feature,np.zeros([1,human_feature],np.int8)),axis=0)\n",
    "        x_test=np.concatenate((x_test[:-3],x_test[1:-2],x_test[2:-1],x_test[3:],feature[2:-1]),axis=1)\n",
    "        np.savez(np_file,x_test=x_test)\n",
    "    return test, x_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(9918438, 43)\n",
      "[0]\ttrain-merror:0.006201\n",
      "[1]\ttrain-merror:0.005233\n",
      "[2]\ttrain-merror:0.004759\n",
      "[3]\ttrain-merror:0.004471\n",
      "[4]\ttrain-merror:0.004253\n",
      "[5]\ttrain-merror:0.004053\n",
      "[6]\ttrain-merror:0.003818\n",
      "[7]\ttrain-merror:0.003616\n",
      "[8]\ttrain-merror:0.003515\n",
      "[9]\ttrain-merror:0.003356\n",
      "[10]\ttrain-merror:0.003241\n",
      "[11]\ttrain-merror:0.003142\n",
      "save model  xgb_model2.dat\n",
      "(956046, 43)\n"
     ]
    }
   ],
   "source": [
    "if __name__=='__main__':\n",
    "    prehead=''\n",
    "    train_data_csv='en_train.csv'\n",
    "    classify_train_file='classify_train.npz'\n",
    "    xgb_model='xgb_model.dat'\n",
    "    test_data_csv='en_test_2.csv'\n",
    "    classify_test_file='classify_test.npz'\n",
    "    xgb_model2='xgb_model2.dat'\n",
    "    classify_test_file2='classify_test2.npz'\n",
    "\n",
    "    # 训练模型\n",
    "    x_train,y_train,label=get_classify_train_data(prehead+classify_train_file,prehead+train_data_csv)\n",
    "    print(x_train.shape)\n",
    "    dtrain = xgb.DMatrix(x_train, label=y_train)\n",
    "    watchlist = [(dtrain, 'train')]\n",
    "    param = {\n",
    "        'eta': 0.3,\n",
    "        'max_depth':10,\n",
    "        'objective':'multi:softmax',\n",
    "        'num_class':len(label),\n",
    "        'eval_metric':'merror',\n",
    "        'subsample': 1,\n",
    "        'colsample_bytree': 1,\n",
    "        'silent':1,\n",
    "        'seed':0,\n",
    "    }\n",
    "    num_boost_rounds=12\n",
    "    model = xgb.train(param, dtrain, num_boost_rounds, watchlist,verbose_eval=1)\n",
    "    print('save model ',xgb_model2)\n",
    "    pickle.dump(model,open(xgb_model2,'wb'))# 保存模型\n",
    "    del x_train,y_train\n",
    "    gc.collect()\n",
    "\n",
    "    # 预测 test 上的 class\n",
    "    model = pickle.load(open(xgb_model2, \"rb\"))\n",
    "    test,x_test=get_classify_test_data(prehead+classify_test_file,prehead+test_data_csv)\n",
    "    print(x_test.shape)\n",
    "    dtest = xgb.DMatrix(x_test)\n",
    "    pred = model.predict(dtest)\n",
    "    pred = [label[int(x)] for x in pred]\n",
    "    test['class']=pred\n",
    "    test.to_csv(os.path.join(prehead, 'test_pred_class_new.csv'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "956046"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "956046"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "956046"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentence_id</th>\n",
       "      <th>token_id</th>\n",
       "      <th>before</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>Last</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>modified</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>2016-03-31</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>There's</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   sentence_id  token_id      before\n",
       "0            0         0        Last\n",
       "1            0         1    modified\n",
       "2            0         2  2016-03-31\n",
       "3            0         3           .\n",
       "4            1         0     There's"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test1 = pd.read_csv('en_test_2.csv')\n",
    "test1.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
